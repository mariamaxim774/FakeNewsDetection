{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-03T18:02:03.750500Z",
     "start_time": "2025-05-03T18:01:58.158160Z"
    }
   },
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import spacy\n",
    "\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def remove_city_newsletter(text):\n",
    "    return re.sub(r\"^[A-Za-z/,\\s]+(?:\\([A-Za-z]+\\))?\\s-\\s\", '', text)\n",
    "\n",
    "\n",
    "def remove_html_caracters(text):\n",
    "    html_special_chars_pattern = r'&[a-zA-Z0-9#]+;|<|>'\n",
    "    return re.sub(html_special_chars_pattern,'',text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http[s]?://\\S+|pic\\.twitter\\.com/\\S+', '', text)\n",
    "\n",
    "\n",
    "def data_lematizing(text): \n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc]\n",
    ")\n",
    "\n",
    "def remove_hastags(text):\n",
    "    return re.sub(r'#\\S+', '', text)\n",
    "  \n",
    "def cleaning_data(text): #removing stopwords and digits and lowercasing them\n",
    "    \n",
    "    text = re.sub('\\n', '', text)\n",
    "    regex_date_username = r\"((january|february|march|april|may|june|july|august|september|october|november|december)\\s\\d{1,2},\\s\\d{4}).*?@(?!realDonaldTrump)\\w+\"\n",
    "\n",
    "    text = re.sub(regex_date_username,'', text, flags=re.IGNORECASE) #eliminarea datelor si numelor utilizatorilor\n",
    "    text=remove_html_caracters(text)\n",
    "    text=remove_city_newsletter(text)\n",
    "    text=remove_links(text)\n",
    "    text=remove_hastags(text)\n",
    "\n",
    "    sentences=tokenizer.tokenize(text)\n",
    "\n",
    "    text=\" \".join(sentence for sentence in sentences if not sentence.startswith(\"Featured\") )\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # elimin semnele de punctuatie\n",
    "    text = re.sub(r\"\\d+\", '', text)  # elimin cifrele\n",
    "    \n",
    "    return \" \".join([w for w in text.split(\" \") if w not in stop_words])\n",
    "\n",
    "\n",
    "def data_tokenization(text): #functia de aici imi separa textul in cuvinte\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def preprocess_data(text):\n",
    "    if isinstance(text, str):\n",
    "        cleaned_data=cleaning_data(text)\n",
    "        lematized_data=data_lematizing(cleaned_data)\n",
    "        return lematized_data\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\github\\AI-examples\\AI_examples\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T18:02:04.763833Z",
     "start_time": "2025-05-03T18:02:03.755486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"data/fake_and_real_news.csv\")"
   ],
   "id": "51a3237f1ac4e479",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-03T18:02:04.948992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "df['clean_text'] = df['text'].progress_apply(preprocess_data)\n",
    "df.to_csv(\"data/cleaned_data_wh_entities.csv\")"
   ],
   "id": "9e384184bb9fdcf6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 21332/44679 [19:01<27:23, 14.20it/s]  "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T18:53:38.209955Z",
     "start_time": "2025-05-03T18:53:36.210760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"data/cleaned_data_wh_entities.csv\")\n",
    "print(df['label'].value_counts(normalize=True))"
   ],
   "id": "a94fab1e2f9b5b9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.525258\n",
      "1    0.474742\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fake_news=df[df['label']==0]\n",
    "real_news=df[df['label']==1]\n",
    "text_fake_news = ' '.join(fake_news['text'].astype(str).tolist())\n",
    "text_real_news = ' '.join(real_news['text'].astype(str).tolist())\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_fake_news)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  \n",
    "plt.title(\"Fake News Word Cloud\")\n",
    "plt.show()\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_real_news)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  \n",
    "plt.title(\"Real News Word Cloud\")\n",
    "plt.show()"
   ],
   "id": "77ff286f35d9c055"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T19:43:21.580924Z",
     "start_time": "2025-04-30T09:42:36.893085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"rajatkumar30/fake-news\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "test_df = pd.read_csv(f\"{path}\\\\news.csv\")\n",
    "test_df['clean_text'] = test_df['text'].apply(preprocess_data)\n",
    "test_df.drop_duplicates()\n",
    "df = df.dropna(subset=['clean_text', 'label'])\n",
    "test_df.to_csv(\"data/cleaned_test_data.csv\")\n"
   ],
   "id": "b1e9b9270e946c97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\User\\.cache\\kagglehub\\datasets\\rajatkumar30\\fake-news\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
